---
title: "DT_CART_2"
author: "LZ"
date: "October 23, 2018"
output: html_document
---

```{r }
library(rpart) # Load CART packages
library(rpart.plot) 
library(caTools) #For Spliting data on Train and Test sets
library(readxl)
library(dplyr)
library(rattle)
library(ROCR)
library(randomForest)


banckloan_cs_noweighths_from_spss <- read_excel("~/Data_mining_2018/banckloan_cs_noweighths_from_spss.xlsx")
our_data_uncleaned<-as.data.frame(banckloan_cs_noweighths_from_spss)#creating data.frame object
#our_data_uncleaned$ed <- factor(our_data_uncleaned$ed, levels = c(1,2,3,4,5), 
#                      labels = c("Did not complete high school", "High school degree", "some college", "college degree", "Post-undergraduate degree"))
#our_data$default_12<- factor(our_data_uncleaned$default, levels= c(0,1), labels=c("No", "Yes")) 
head(our_data_uncleaned)
our_data<-our_data_uncleaned[,-(1:3)]
head(our_data)
str(our_data)

#________________classification______________

table(our_data$default)

   #creating test and train sets
   set.seed(100)
   split_1 = sample.split(our_data$default, SplitRatio = 0.8)
   b_train = subset(our_data, split_1==TRUE)
   b_test = subset(our_data, split_1==FALSE)
  dim(b_train);  dim(b_test)
  #prop.table(table(b_train$default)) # to be sure that randomization is correct
  #prop.table(table(b_test$default))  #proportion is same in both datasets
  
  #model + graphs
  cart_1<-rpart(formula=default~., data = b_train, method = "class", parms=list(split="gini")) #method="class" for classification and anova for regression # The splitting index can be gini or information. The default to gini.
  rpart.plot(x=cart_1,type = 1, extra = 104, nn=TRUE, box.palette = "YlGnBl" #,  ni = TRUE, branch=0,4  ##BnPu #show.prp.palettes
                       ) #type=design, nn:node number, extra: how to display information #for extra see https://cran.r-project.org/web/packages/rpart.plot/rpart.plot.pdf
        
  
        par(mfrow=c(1,2))
        prp(cart_1,type=2, extra=2,faclen=3, # lenght of the factor variable to be shown 
             main="Decision tree for Bank_loan") 
  
        fancyRpartPlot(cart_1)# Vizualizing with rattle
        par(mfrow=c(1,1))
        
        
  # Decision tree as set of rules
  asRules(cart_1)
  print(cart_1)#print results
  #summary(cart_1)#detailed results including surrogate splits
  
  #make a prediction
  predicted_<-predict(cart_1, b_test, type = "class")
  confusion_mat<-table(Actual=b_test$default, Predicted= predicted_)
  confusion_mat
  # measure performance #accurancy
  accuracy_test<-sum(diag(confusion_mat))/sum(confusion_mat)
  accuracy_test
  #replica same for train set...
  fit_<-predict(cart_1, b_train, type = "class")
  confusion_mat_for_train<-table(Actual=b_train$default, Predicted= fit_)
  confusion_mat_for_train
  accuracy_train<-sum(diag(confusion_mat_for_train))/sum(confusion_mat_for_train)
  accuracy_train
  
  
  # Take the probability of defaults
  pred_prob<-predict(cart_1, newdata =  b_test, type="prob") #???????
  pred <- prediction(pred_prob[,2],b_test$default) 
  perf <- performance(pred,"tpr","fpr")
  performance(pred, "auc")@y.values
  plot(perf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
  
  #why the model is overfitted
  A<-matrix(nrow=30,ncol=2)
  
  for (i in 1:30) {
    
    cart_over<-rpart(default~., data=b_train, method="class",
                      control= rpart.control(maxdepth = i ))
    predicted_over<-predict(cart_over, b_test, type = "class")
    accuracy_test<-sum(diag(table(Actual=b_test$default, Predicted= predicted_over)))/sum(table(Actual=b_test$default, Predicted= predicted_over))
    
    fit_over<-predict(cart_over, b_train, type = "class")
    accuracy_train<-sum(diag(table(Actual=b_train$default, Predicted= fit_over)))/sum(table(Actual=b_train$default, Predicted= fit_over))
    
    A[i,1]<-accuracy_test
    A[i,2]<-accuracy_train
  }
  par(mfrow=c(1,2))
  plot( seq(1,30),A[,2], type="o", col="red", ylab="train", xlab="maxdepth")
  plot( seq(1,30),A[,1], type="o", col="blue", ylab="test", xlab="maxdepth")
  par(mfrow=c(1,1))
  # Tune the model # create controls
  
  cart_tuned<-rpart(default~., data=b_train, method="class",
             control= rpart.control(minsplit=100, minbucket=150)) # maxdepth = 3, 
  #minbucket: the minimum number of observations in any terminal <leaf> node. If only one of minbucket or minsplit is specified, the code either sets minsplit to minbucket*3 or minbucket to minsplit/3, as appropriate.
  #minsplit:the minimum number of observations that must exist in a node in order for a split to be attempted.
    rpart.plot(cart_tuned, extra = 101, nn=TRUE)
  
  
  #_________Pruning______ #prune the tree
  set.seed(1600000)
  #complexity parameter. Any split that does not decrease the overall lack of fit by a factor of cp is not attempted.
  #For instance, with anova splitting, this means that the overall R-squared must increase by cp at each step.
  #Validation of decision tree using the 'Complexity Parameter' and cross validated error
  
  #Prints a table of optimal prunings based on a complexity parameter.
  # multiply columns 3-5 by 483 to get a result in terms of absolute error. 
  printcp(cart_1) 
```
```{r}
  438*cart_1$cptable[6,"rel error"]
  confusion_mat_for_train[2,1]+confusion_mat_for_train[1,2]
  k=0.365*0.72831*100 #cross
  paste("Misisclassification rate of", k, "in cross-validation: prediction accuracy:", 100-k )
  
  plotcp(cart_1)
  pr_<-prune(cart_1, cp=cart_1$cptable[which.min(cart_1$cptable[,"xerror"]),"CP"])
  printcp(pr_)
  
  par(mfrow=c(1,2))
  fancyRpartPlot(cart_1)
  fancyRpartPlot(pr_,main="Pruned Classification Tree")
  par(mfrow=c(1,1))
  
  #Random forest____1
  
  set.seed(4150)
  default_rf <- as.character(b_train$default)
  default_rf <- as.factor(default_rf)
  default_rf_test <- as.character(b_test$default)
  default_rf_test <- as.factor(default_rf_test)
  
  fit.rf<-randomForest(default_rf~debtinc+emloy+creddebt+income, data=b_train, type =" classification") #mtry, #ntree #replace
  print(fit.rf)
  #number f variables tried at each split: 2
  #if we want to change it we should change the argument mtry=4(wich will be bagging)
  #by default mtry=p/3 for regression, sqrt(p) for class
  #?randomForest
  #plot(fit.rf)
  #Now, how the bagged model perform on the test data
  
  
  #Testing
  predict1<-predict(fit.rf, newdata = b_test, type="class")
  plot(predict1, default_rf_test)
  abline(0,1)
  CM1<-table(Actual=default_rf_test, Predicted=predict1)
  acc1<-sum(diag(CM1))/sum(CM1)
  acc1
  pred_prob1<-predict(fit.rf, b_test, type="prob")
  pred1 <- prediction(pred_prob1[,2],
                      default_rf_test) 
  perf1 <- performance(pred1,"tpr","fpr")
  performance(pred1, "auc")@y.values
  plot(perf1, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
  
  #we also can change the number of trees: ntree
  
  #Random forest____2____BAGGING
  fit.rf_2<-randomForest(default_rf~debtinc+emloy+creddebt+income, 
                       data=b_train, type =" classification", mtry=4) 
  print(fit.rf_2)
  
  predict2<-predict(fit.rf_2, newdata = b_test, type="class")
  CM2<-table(Actual=default_rf_test, Predicted=predict2)
  acc2<-sum(diag(CM2))/sum(CM2)
  acc2
  pred_prob2<-predict(fit.rf_2, b_test, type="prob")
  pred2 <- prediction(pred_prob2[,2],
                      default_rf_test) 
  perf2 <- performance(pred2,"tpr","fpr")
  performance(pred2, "auc")@y.values
  plot(perf2, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
  #2-i auc poqracav RF have good resulr than bagging in this case
  importance(fit.rf)
  varImpPlot (fit.rf)

  ##______________________BOOSTING_______________________
  #library(gbm)
  #library(fastAdaboost)
  library(adabag)
  set.seed(1)
  b_train$default<-as.factor(b_train$default)
  boost.default<-boosting(default~debtinc+emloy+creddebt+income, b_train,
                          boos = TRUE, mfinal = 100, coeflearn = 'Breiman')
 
  summary(boost.default)
  head(boost.default$trees)
  errorevol(boost.default, b_train) #error evolution
  
  pred.boost<-predict(boost.default,
                      newdata = b_test
                      )
par(mfrow=c(1,1))  
  pred.boost$class
  pred.boost$formula
  head(pred.boost$prob)
  pred.boost$confusion
  tree1<-boost.default$trees[[1]]
  library(tree)
  plot(tree1)  
  text(tree1, pretty=0)
  
  
```

```{r}
#===========Regression=======
#something like >=mean of dependent variable (reg_tree)
summary(our_data$creddebt)
plot(our_data$debtinc, our_data$age)
points(our_data$debtinc[our_data$creddebt>=1.935], our_data$age[our_data$creddebt>=1.935], col="green", pch=19)

reg_tree_1 = rpart(creddebt ~ ed+age+debtinc+default, data=our_data)
prp(reg_tree_1)
#pruning by changing minbucket size 
reg_tree_2 = rpart(creddebt ~ ed+age+debtinc+default, data=our_data, minbucket=50)
prp(reg_tree_2)
plot(reg_tree_2)
text(reg_tree_2)

#train-test
set.seed(1500)
split = sample.split(our_data$creddebt, SplitRatio = 0.7)
train = subset(our_data, split==TRUE)
test = subset(our_data, split==FALSE)

reg_tree_3 = rpart(creddebt ~ ed+age+debtinc+default, 
                   data=train)
prp(reg_tree_3)
#PREDICTION
tree_pred = predict(reg_tree_3, newdata=test)
tree_sse = sum((tree_pred - test$creddebt)^2)
tree_sse

rsq.rpart(cart_1)#plot approximate R-squared and relative error for different splits (2 plots). labels are only appropriate for the "anova" method.
```
